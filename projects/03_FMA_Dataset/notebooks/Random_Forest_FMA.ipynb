{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:26:04.452265Z",
     "start_time": "2025-08-13T01:26:04.447664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ### Step 0: Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ],
   "id": "5b87474717e74732",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:26:08.526036Z",
     "start_time": "2025-08-13T01:26:04.485550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ### Step 1: Load the Data\n",
    "print(\"Loading data...\")\n",
    "# Make sure your CSV files are in the same directory as this notebook, or provide the full path.\n",
    "tracks_path = '../dataset/tracks.csv'\n",
    "features_path = '../dataset/features.csv'\n",
    "\n",
    "# Load the data with the correct multi-level headers\n",
    "tracks = pd.read_csv(tracks_path, index_col=0, header=[0, 1])\n",
    "features = pd.read_csv(features_path, index_col=0, header=[0, 1, 2])\n",
    "print(\"Data loaded successfully.\")"
   ],
   "id": "3a4b45006625040e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded successfully.\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:26:08.601263Z",
     "start_time": "2025-08-13T01:26:08.546693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ### Step 2: Prepare the DataFrame (The Definitive, Robust Method)\n",
    "print(\"Preparing the dataset with a robust pipeline...\")\n",
    "\n",
    "# 1. Get the track IDs for the 'small' subset from the tracks metadata.\n",
    "small_tracks_ids = tracks[tracks[('set', 'subset')] == 'small'].index\n",
    "\n",
    "# 2. Find the intersection of track IDs that exist in BOTH the features and the small_tracks list.\n",
    "# This guarantees our data is perfectly aligned.\n",
    "common_ids = features.index.intersection(small_tracks_ids)\n",
    "\n",
    "# 3. Filter both DataFrames to only include these common, perfectly matched tracks.\n",
    "small_features = features.loc[common_ids]\n",
    "small_genres = tracks.loc[common_ids][('track', 'genre_top')]\n",
    "\n",
    "# 4. Create our final DataFrame for modeling.\n",
    "df_final = small_features.copy()\n",
    "\n",
    "# --- THIS IS THE KEY FIX ---\n",
    "# 5. Flatten the multi-level column headers from the features FIRST.\n",
    "df_final.columns = ['_'.join(str(c) for c in col).strip() for col in df_final.columns.values]\n",
    "\n",
    "# 6. NOW, add the genre column to the DataFrame that has simple columns.\n",
    "#    Using .values helps prevent potential index misalignment issues.\n",
    "df_final['genre'] = small_genres.values\n",
    "# --- END OF FIX ---\n",
    "\n",
    "print(\"Dataset prepared correctly.\")\n",
    "print(f\"Shape of our modeling dataset: {df_final.shape}\")\n",
    "\n"
   ],
   "id": "5c60807896bd715",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the dataset with a robust pipeline...\n",
      "Dataset prepared correctly.\n",
      "Shape of our modeling dataset: (8000, 519)\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:26:08.629853Z",
     "start_time": "2025-08-13T01:26:08.625364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ### Step 3: Define Features (X) and Target (y)\n",
    "# This will now work correctly because the 'genre' column exists.\n",
    "X = df_final.drop('genre', axis=1)\n",
    "y = df_final['genre']"
   ],
   "id": "b7360224d68b481b",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:26:08.675501Z",
     "start_time": "2025-08-13T01:26:08.652308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ### Step 4: Preprocessing (Encoding, Splitting, Scaling)\n",
    "print(\"Preprocessing data...\")\n",
    "# Encode the Genre Labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Scale the Features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ],
   "id": "74862080d050ad6d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data...\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:26:09.697722Z",
     "start_time": "2025-08-13T01:26:08.703230Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ### Step 5: Train and Evaluate the Random Forest Model\n",
    "print(\"Training the Random Forest model...\")\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "print(\"Model training complete.\")"
   ],
   "id": "1cd6ffd75f0a582a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Random Forest model...\n",
      "Model training complete.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-13T01:26:09.751607Z",
     "start_time": "2025-08-13T01:26:09.731808Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ### Step 6: Evaluate the Model\n",
    "print(\"\\n--- Model Evaluation ---\")\n",
    "predictions = model.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Final Model Accuracy with Correct Pipeline: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, predictions, target_names=label_encoder.classes_))\n",
    "\n"
   ],
   "id": "7fbb9f6ebeb9bdbe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Model Evaluation ---\n",
      "Final Model Accuracy with Correct Pipeline: 55.81%\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Electronic       0.59      0.57      0.58       200\n",
      " Experimental       0.53      0.40      0.46       200\n",
      "         Folk       0.62      0.75      0.68       200\n",
      "      Hip-Hop       0.58      0.67      0.62       200\n",
      " Instrumental       0.56      0.58      0.57       200\n",
      "International       0.60      0.60      0.60       200\n",
      "          Pop       0.34      0.28      0.31       200\n",
      "         Rock       0.58      0.62      0.60       200\n",
      "\n",
      "     accuracy                           0.56      1600\n",
      "    macro avg       0.55      0.56      0.55      1600\n",
      " weighted avg       0.55      0.56      0.55      1600\n",
      "\n"
     ]
    }
   ],
   "execution_count": 27
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
